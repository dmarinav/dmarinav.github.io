---
title: "Clustering flights delays"
excerpt: "The aim of this post is to identify meaningful clusters of flight delays for FAA regulated airports in the US"
header:
  overlay_image: air_logo5.jpg
  overlay_filter: 0.4
  caption: " "
  cta_label: "Next Post"
  cta_url: "https://dmarinav.github.io/data%20science/the-hot-100-how-hot-it-is/"
categories:
  - data science
tags:
  - hierarchical clustering
  - principal component analysis 
  - k-means clustering
  - silhouette score
  - DBSCAN
author: "Marina Drus"
date: "15 November 2016"
---

{% include base_path %}

### Introduction

In this project, the main goal was to identify meaningful and useful (sharing common characteristics) clusters of flight delays for FAA regulated airports in the US. Data included name of airport, state, city, operational year (from 2004 to 2010), departures, arrivals, cancelations, numbers of delays and diversions, average delay times for a number of operational processes (such as taxi-out, taxi-in, airborne time, etc.). Please see detailed description of the features [here](http://aspmhelp.faa.gov/index.php/APM:_Analysis:_Definitions_of_Variables). Because some of the features were measured in minutes and some were measured in percentage, the data was normalized (scaled) before the analyses was conducted. 


### Principal Components Analysis


[Principle components analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)(PCA) is dimension-reduction technique that is completely different from clustering methods such k-means clustering and hierarchical clustering. PCA seeks to find the best possible low-dimensional representation that explains the maximum amount of variance. Clustering (whether it is k-means analysis or agglomerative hierarchical analysis) looks for similar subgroups in observation data. Before conducting PCA, I needed to see correlations between the features to determine whether features have large enough relationships with each other.


![Correlational Table]({{ site.url }}{{ site.baseurl }}/images/pic5_1.png){: .align-center} 

As the correlational table indicates, features that reflect measures of airport delays are highly correlated among each other.

Next, I identified eigenvalues in my PCA model. 


[![Eigenvalues]({{ site.url }}{{ site.baseurl }}/images/pic5_2.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_2.png)


The analysis distinguished 17 eigenvalues. The very first eigenvalue had the largest value, while the first three eigenvalues indicated values larger than 1. 

As seen below, the first three eigenvalues formed three major components that that cumulatively explained 85.51% of variance for airport delays. PC1 explained 53.91% of all airport delays while PC1 and PC2 together explained 78.37% of all airport delays. 


[![Major Components]({{ site.url }}{{ site.baseurl }}/images/pic5_3.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_3.png)


I used Kaiser’s stopping rule to select the first three components. Kaiser’s stopping rule states that only the number of factors with eigenvalues over 1.00 should be considered in the analysis. However, this was an arbitrary decision. In this case, I am solely interested in identifying meaningful and useful clusters. In some way, PCA is helping me to clean my clusters from the features that are less likely to share common characteristics with major meaningful clusters. In other cases, different rules to selecting major components from PCA would be applied. 

As seen in the table below, PC1 is positively correlated with all the features included in the PCA except on-time departure and arrival. Here, PC1 is negatively correlated with the features. PC2 is positively correlated only with several features such as gate delay, air departure delay, gate arrival delay and is negatively correlated with dept_comp, arriv_comp, ontime_gate_dept, ontime_air_dept, ontime_gate_arriv, taxi_in_delay,dept_cancel,arriv_cancel, and arriv_diversions. PC3 is positively correlated with on-time gate departure, taxi out time, taxi out delay, airborn delay, and block delay and is negatively correlated with ontime_gate_dept, taxi_out_time, taxi_out_delay, airborn_delay, and block_delay.


| Feature           | PC1           |      PC2      |PC3           | 
|:-----------------:|:-------------:|:-------------:|:-------------:
|dept_comp          | .78           |-.55           |.09           |
|arriv_comp         | .78           |-.56           |.09           |
|ontime_gate_dept   |-.50           |-.71           |-.36          |
|ontime_air_dept    |-.83           |-.47           |.002          |
|ontime_gate_arriv  |-.45           |-.82           |-.10          |
|gate_delay         |.61            |.70            |.28           |
|taxi_out_time      |.82            |-.08           |-.46          |
|taxi_out_delay     |.83            |-.04           |-.44          |
|air_dept_delay     |.85            |.48            |-.01          |
|airborn_delay      |.61            |.11            |-.53          |
|taxi_in_delay      |.81            |-.36           |.05           |
|block_delay        |.67            |.24            |-.32          |
|gate_arrive_delay  |.64            |.68            |.04           |
|dept_cancel        |.82            |-.36           |.23           |
|arriv_cancel       |.81            |-.38           |.24           |
|dept_diversions    |.75            |-.55           |.19           |
|arriv_diversions   |.77            |-.42           |.19           |


### K-means Analysis

K-means analysis is a clustering technique that seeks to group observations into a pre-existing number of clusters. In this type of clustering analysis, researchers have to specify the desired number of clusters. Here, several k-means analyses were conducted with a various number of clusters. Also, k-means were conducted on original data and PCA-transformed data (the first three components identified in PCA). 


[![Inertia Values]({{ site.url }}{{ site.baseurl }}/images/pic5_4.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_4.png)


Inertia values, a measure of how internally coherent clusters are, indicate that PCA-reduced clusters have less distortion that the original clusters do, and, in general, distortion decreases with a number of clusters. Overall, the high inertia values show a very high distortion in the clusters (ideally, the less this number is the less the distortion is), indicating that clusters might be too cohesive with each other.  Since [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) tends to become inflated (a so-called [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)), running a PCA prior to k-means alleviates this problem. 
 

Another measure of k-means performance is the [Silhouette coefficient](https://en.wikipedia.org/wiki/Silhouette_(clustering). The Silhouette coefficient is calculated using the mean intra-cluster distance and the mean nearest-cluster distance for each sample. Below are the Silhouette plots for k-means analyses with 2 and 3 clusters for original and PCA-transformed data. In general, we care not only for the average value of the Silhouette coefficient, but also for wide fluctuations in the size of the silhouette plots. 


[![Silhouette plot 1]({{ site.url }}{{ site.baseurl }}/images/pic5_5.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_5.png)

[![Silhouette plot 2]({{ site.url }}{{ site.baseurl }}/images/pic5_6.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_6.png)


[![Silhouette plot 3]({{ site.url }}{{ site.baseurl }}/images/pic5_7.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_7.png)

[![Silhouette plot 4]({{ site.url }}{{ site.baseurl }}/images/pic5_8.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_8.png)


Even though the average value of the Silhouette coefficient increases for the PCA-transformed data, it is still relatively low (for 2 clusters: 0.44 vs 0.49, and for 3 clusters: 0.25 vs. 0.31). Because of the distortions in the original data, I had to represent the Silhouette plots for the original data in the space of 2nd and 3rd features to see the clusters clearly. As the number of k-means clusters increases, the Silhouette coefficient decreases as well. In addition, huge fluctuations in the size of the silhouette plots signals the presence of low homogeneity among clusters, supporting the findings of the high inertia values. Moreover, the negative values of the Silhouette coefficient indicate the presence of outliers. For the further analysis, I picked k-means results with 2 clusters for the PCA-transformed data. The 2 PCA-transformed k-means clusters give us the highest Silhouette coefficient and the reasonable, compared to the others, inertia value (.49 and 6829.64, correspondingly). 


### Presenting K-means Clusters in Principle Component Space

Further, it was interesting to see how k-means clusters lie in the principle component space. In two dimensional space, I plotted k-means clusters along with annotated clusters to see which airport belongs to any given k-means cluster. 


[![PCA and K-means1]({{ site.url }}{{ site.baseurl }}/images/pic5_9.png){:height="2000px" width="960px"}]({{ site.url }}{{ site.baseurl }}/images/pic5_9.png)


[![PCA and K-means2]({{ site.url }}{{ site.baseurl }}/images/pic5_10.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_10.png)


[![PCA and K-means3]({{ site.url }}{{ site.baseurl }}/images/pic5_11.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_11.png)


While OXR, RFD, MDW, OGG, SEA and etc. experience one type of delays and belong to the k-means cluster 1, JFK, EWR, LGA, PHL, ATL, ORD, DFW and others seem to experience a different type of delays since they belong to the k-means cluster 2. Below is the same k-means clusters but plotted in the 3-D principle component space.


[![PCA and K-means in 3D]({{ site.url }}{{ site.baseurl }}/images/pic5_12.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_12.png)


### DBSCAN and identifying outliers.

[DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) is another useful clustering algorithm, which stands for Density-Based Spatial Clustering of Applications with Noise. There are several advantages of using DBSCAN: 1) it does not require to know the number of clusters in advance, 2) it can capture clusters of very complex shapes, and 3) most importantly, it is extremely useful in identifying outliers or points that are not part of any cluster.


[![DBSCAN]({{ site.url }}{{ site.baseurl }}/images/pic5_13.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_13.png)


The DBSCAN analysis with min_samples=10 and eps=2.4 yielded the highest Silhouette Coefficient of 0.63 and identified only one cluster with 11 outliers. Those were: ATL(2004), ATL(2005), ATL (2006), JFK(2007), EWR(2008), HPN(2009), ORD(2010), ORD(2011), ORD(2012), ORD(2013), ORD(2014). 


### Agglomerative Hierarchical Clustering


[Hierarchical clustering](9 https://en.wikipedia.org/wiki/Hierarchical_clustering) is another approach that, just like DBSCAN, does not require to have a predetermined number of clusters. One advantage of hierarchical clustering over k-means clustering in that it produces a dendrogram, a tree-based representation of the observations. The algorism looks for dissimilarities between a pair of observations. The concept of dissimilarity is extended to a pair of groups of observations, and this extension is achieved by introducing the notion of linkage which detects the dissimilarity between the two groups. Average and complete linkages are usually preferred over single linkage. Complete linkage measures the farthest pair of points, and average linkage measures the average dissimilarity over all pairs. Because complete linkage can have problems with crowding (and the observations here are pretty crowded), I used average linkage in addition to complete linkage.


[![Dendrogram]({{ site.url }}{{ site.baseurl }}/images/pic5_14.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_14.png)


It looks like complete linkage behaves similarly to k-means analysis and average linkage behaves similarly to DBSCAN. Indeed, we can see that average linkage identified an outlier (observation #609 – HPN in 2009). The same outlier (and much more) was observed in the DBSCAN analysis. In addition, average linkage yielded a much higher [Cophenetic correlation]( https://en.wikipedia.org/wiki/Cophenetic_correlation) than the complete linkage (0.83 vs. 0.67).

To illustrate my point better, I presented the observations in two-dimensional space and compare them with the k-means clusters.


[![PCA ns K-means]({{ site.url }}{{ site.baseurl }}/images/pic5_15.png)]({{ site.url }}{{ site.baseurl }}/images/pic5_15.png)


As seen above, the hierarchical clustering with complete linkage forms two clusters – just like in the case of the k-means analysis. On the other hand, the hierarchical clustering with average linkage forms one cluster and one outlier – just like in case of the DBSCAN analysis. In reality, it forms two clusters, but because second cluster consists only of one observation, average linkage method serves here as an outlier detector. 


### Conclusion

The clustering techniques above identified two clusters and several outliers. First, the principal component analysis identified 3 major components (PC1, PC2, and PC3) that cumulatively explained 85% of variance for airport delays. PC1 explained 54% of all airport delays while PC1 and PC2 together explained 73% of all airport delays. K-means analysis identified two major clusters. However, a relatively low the Silhouette coefficient and high distortion signaled a presence of cohesiveness among the clusters. Performing k-means cluster analysis on PCA-transformed data improved the scores and slightly decreased the cohesiveness. DBSCAN analysis revealed only one major cluster with several outliers. Those were: ATL(2004), ATL(2005), ATL (2006), JFK(2007), EWR(2008), HPN(2009), ORD(2010), ORD(2011), ORD(2012), ORD(2013), ORD(2014).

The same phenomenon was observed in hierarchical clustering. Complete vs. average linkage method yielded lower Cophenetic correlation, indicating a presence of high cohesion among clusters. Average linkage behaved identically to DBSCAN and identified one cluster and one outlier, HPN(2009). 

The evidence above indicates that USA flight delays form two meaningful and useful clusters. K-means and hierarchical clustering with average linkage identified these clusters. However, the clusters also have some cohesiveness with each other. That is why DBSCAN and hierarchical clustering with average linkage identify only one cluster and outliers. Possibly, getting rid of outliers will only result in one meaningful cluster. 































