---
title: "Clustering flights delays: Is your flight likely to delay?"
excerpt: "The aim of this post is to identify meaningful clusters of flight delays for FAA regulated airports in the US"
header:
  overlay_image: air_logo5.jpg
  overlay_filter: 0.4
  caption: ""
  cta_label: " "
categories:
  - data science
tags:
  - hierarchical clustering
  - principal component analysis 
  - k-means clustering
  - silhouette score
  - DBSCAN
author: "Marina Drus"
date: "15 November 2016"
---

{% include base_path %}

### Introduction

In this project, the main goal was to identify meaningful and useful (sharing common characteristics) clusters of flight delays for FAA regulated airports in the US. Data included name of airport, state, city, operational year (from 2004 to 2010), departures, arrivals, cancelations, numbers of delays and diversions, average delay times for a number of operational processes (such as taxi-out, taxi-in, airborne time, etc.). Please see detailed description of the features [here](http://aspmhelp.faa.gov/index.php/APM:_Analysis:_Definitions_of_Variables). Because some of the features were measured in minutes and some were measured in percentage, the data was normalized (scaled) before the analyses was conducted. 


### Principal Components Analysis


Principle components analysis (PCA) is dimension-reduction technique that is completely different from clustering methods such k-means clustering and hierarchical clustering. PCA seeks to find the best possible low-dimensional representation that explains the maximum amount of variance. Clustering (whether it is k-means analysis or agglomerative hierarchical analysis) looks for similar subgroups in observation data. Before conducting PCA, I needed to see whether features have large enough relationships with each other to conduct PCA. As seen below from the correlational table, features that reflect measures of airport delays are highly correlated among each other.


![Correlational Table]({{ site.url }}{{ site.baseurl }}/images/pic5_1.png){: .align-center} 


Next, I identified eigenvalues in my PCA model. The analysis distinguish 17 eigenvalues. The very fisrt eigenvalue had the largest value, while the fisrt three eigenvalues indicated values larger than 1. 


![Eigenvalues]({{ site.url }}{{ site.baseurl }}/images/pic5_2.png){: .align-center}


As seen below, the first three eigenvalues formed three major components that that cumulatively explained 85.51% of variance for airport delays. PC1 explained 53.91% of all airport delays while PC1 and PC2 together explained 78.37% of all airport delays. I used Kaiser’s stopping rule to select the first three components. Kaiser’s stopping rule states that only the number of factors with eigenvalues over 1.00 should be considered in the analysis. However, this was an arbitrary decision. In this case, I am solely interested in identifying meaningful and useful clusters. In some way, PCA is helping me to clean my clusters from the features that are less likely to share common characteristics with major meaningful clusters. In other cases, different rules to selecting major components from PCA would be applied. 


![Major Components]({{ site.url }}{{ site.baseurl }}/images/pic5_3.png){: .align-center}


As seen in the table below, PC1 is positively correlated with all the features included in the PCA except on-time departure and arrival. Here, PC1 is negatively correlated with the features. PC2 is positively correlated only with several features such as gate delay, air departure delay, gate arrival delay and is negatively correlated with dept_comp, arriv_comp, ontime_gate_dept, ontime_air_dept, ontime_gate_arriv, taxi_in_delay,dept_cancel,arriv_cancel, and arriv_diversions. PC3 is positively correlated with on-time gate departure, taxi out time, taxi out delay, airborn delay, and block delay and is negativelly correlated with ontime_gate_dept, taxi_out_time, taxi_out_delay, airborn_delay, and block_delay.


| Feature           | PC1           |      PC2      |PC3           | 
|:-----------------:|:-------------:|:-------------:|:-------------:
|dept_comp          | .78           |-.55           |.09           |
|arriv_comp         | .78           |-.56           |.09           |
|ontime_gate_dept   |-.50           |-.71           |-.36          |
|ontime_air_dept    |-.83           |-.47           |.002          |
|ontime_gate_arriv  |-.45           |-.82           |-.10          |
|gate_delay         |.61            |.70            |.28           |
|taxi_out_time      |.82            |-.08           |-.46          |
|taxi_out_delay     |.83            |-.04           |-.44          |
|air_dept_delay     |.85            |.48            |-.01          |
|airborn_delay      |.61            |.11            |-.53          |
|taxi_in_delay      |.81            |-.36           |.05           |
|block_delay        |.67            |.24            |-.32          |
|gate_arrive_delay  |.64            |.68            |.04           |
|dept_cancel        |.82            |-.36           |.23           |
|arriv_cancel       |.81            |-.38           |.24           |
|dept_diversions    |.75            |-.55           |.19           |
|arriv_diversions   |.77            |-.42           |.19           |


### K-means Analysis

K-means analysis is a clustering technique that seeks to group observations into a pre-existing number of clusters. In this type of clustering analysis, researchers have to specify the desired number of clusters. Here, several k-means analyses were conducted with a various number of clusters. Also, k-means were conducted on original data and PCA-transformed data (the first three components identified in PCA). Inertia values, a measure of how internally coherent clusters are, below indicate that PCA-reduced clusters have less distortion that the original features do, and, in general, disortion decreses with a number of clusters. Overall, the high inertia values show a very high distortion in the clusters (ideally, the less this number is the less the distortion is), indicating that clusters are far from homogeneous.  Since Euclidean distances tend to become inflated (which is called a “curse of dimensionality”), running a PCA prior to k-means alleviates this problem. 

![Inertia Values]({{ site.url }}{{ site.baseurl }}/images/pic5_4.png){: .align-center}

Another measure of k-means performance is the Silhouette coefficient. The Silhouette coefficient is calculated using the mean intra-cluster distance and the mean nearest-cluster distance for each sample. Below are the Silhouette plots for k-means analyses with 2 and 3 clusters for original and PCA-transformed data. In general, we care not only for the average value of the Silhouette coefficient, but also for wide fluctuations in the size of the silhouette plots. 

As seen on the Silhouette plots, even though the average value of the Silhouette coefficient increases for the PCA-transformed data, it is still relatively low (for 2 clusters: 0.44 vs 0.49, and for 3 clusters: 0.25 vs. 0.31). Because of the distortions in the original data, I had to represent the Silhouette plots for the original data in the space of 2nd and 3rd features to see the clusters clearly. As the number of k-means clusters increases, the Silhouette coefficient decreases as well. In addition, huge fluctuations in the size of the silhouette plots signals the presence of low homogeneity among clusters, supporting the findings of the high inertia values. Moreover, the negative values of the Silhouette coefficient indicate the presence of outliers. For the further analysis, I picked k-means results with 2 clusters for the PCA-transformed data. The 2 PCA-transformed k-means clusters give us the highest Silhouette coefficient and the reasonable, compared to the others, inertia value (.49 and 6829.64, correspondingly). 



![Silhouette plot 1]({{ site.url }}{{ site.baseurl }}/images/pic5_5.png){: .align-center}

![Silhouette plot 2]({{ site.url }}{{ site.baseurl }}/images/pic5_6.png){: .align-center}

![Silhouette plot 3]({{ site.url }}{{ site.baseurl }}/images/pic5_7.png){: .align-center}

![Silhouette plot 4]({{ site.url }}{{ site.baseurl }}/images/pic5_8.png){: .align-center}


### K-means Clusters in Principle Component Space

It was interesting to see how k-means clusters lie in the principle component space. I plotted k-means clusters along with annotated clusters to see which aircraft carriers belongs to a given k-means cluster. JFK, EWR, LGA, PHL, ATL, ORD, and DFW seem to experience a different type of delays since they belong to the Cluster 2. 

![PCA ns K-means]({{ site.url }}{{ site.baseurl }}/images/pic5_9.png){: .align-center}

![PCA ns K-means]({{ site.url }}{{ site.baseurl }}/images/pic5_10.png){: .align-center}

![PCA ns K-means]({{ site.url }}{{ site.baseurl }}/images/pic5_11.png){: .align-center}

![PCA ns K-means]({{ site.url }}{{ site.baseurl }}/images/pic5_12.png){: .align-center}


























