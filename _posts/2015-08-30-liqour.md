---
title: "Building a Profitability Model for an Iowa Liqour Store"
excerpt: "This post covers a predictive model for Iowa liqour stores"
header:
  overlay_image: liqour2.jpg
  overlay_filter: 0.4
  caption: ""
  cta_label: " "
categories:
  - data science
tags:
  - profitability model
  - multiple regression
  - linear regression 
  - decision tree regressor
  - linear regression assumptions
author: "Marina Drus"
date: "30 October 2016"
---

{% include base_path %}

### Introduction

The goal of this project was to build a profitability model based on total spirits sales in Iowa of individual products at the store level. Since it is a very large data set, only ten percent of the data was retrieved from [the Iowa website](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy). The dataset contains the spirits purchase information of Iowa Class “E” liquor licensees by product and date of purchase. For the variables’ description, please visit [the Iowa website](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy).

The data contains total sale of liquor ordered per transaction bought by each store from the state of Iowa. That is right. The data does not contain total sale of liquor for each transaction made by customers in each store. Thus, while exploring the data, a very important assumption was made that owners in Iowa are knowledgeable business people, and they only stock what they sell. I really hope so, otherwise they would probably go out of business. In addition to the data I had, two new variables were created: Store (a number of stores per each zip code) and Vendor (a number of vendors per each zip code).


### Multiple Regression Model

Since the outcome variable Sale is continuous, a multiple regression model was built. I used stepwise approach in building the regression model. However, first and foremost, I looked at average sales per transaction that are outliers. As seen below from the histogram and Q-Q plot, average costs per transaction that exceed $250 were exceptions. 


![Histogram and QQ Plot With Outliers]({{ site.url }}{{ site.baseurl }}/images/pic2_1.png){: .align-center} 


After removing the outliers, the distribution of average cost per transaction looked much closer to normal distribution. See the histogram and Q-Q plot below.


![Histogram and QQ Plot Without Outliers]({{ site.url }}{{ site.baseurl }}/images/pic2_2.png){: .align-center} 


Finally, I got to conduct a stepwise regression to select my final linear regression model. Stepwise regression is a method of fitting regression models step-by-step. In each step, I consider a variable for addition to or subtraction from the set of all variable based on an adjusted R-squared.  If saw no changes in an adjusted R-squared, I would not include that variable into the statistical model. I also looked at multicollearity, the highly correlated independent variables were removed from the model. I excluded variable Cost (cost per transaction) from my data set as it was the same as variable Retail (retail price per transaction) but multiplied by Sold (a number of bottles sold), so naturally I expected Cost and Retail to be highly correlated. Because Store (a number of stores per each zip code) and Vendor (a number of vendors per each zip code) highly correlated (r=.73), I removed Store out of my model. I also checked whether Store affected in anyway the adjusted R-squared and found no changes there. 


![Correlational Matrix]({{ site.url }}{{ site.baseurl }}/images/pic2_3.png){: .align-center}


Since the variable Month did not predict Sale, the final model excluded the month when liquor was purchased. Consequentially, I adjusted density of the data points by removing category Month from my data. As seen next, regrouping the data substantially decreased MSE (mean squared error) of the final regression model.


![Scatters for Density]({{ site.url }}{{ site.baseurl }}/images/pic2_4.png){: .align-center}


As seen in the regression table below, all independent variables positively and significantly predicted average sale per transaction. Sale per transaction increased as Retail (retail price per bottle), Sold (a number of bottles sold), Vendor (a number of vendors per zip code), and Volume (bottle volume) increased. All categories of liquor (expect schnapps) significantly predicted sale per transaction compared to brandy (the baseline category). 


![Regression Table]({{ site.url }}{{ site.baseurl }}/images/table2_1.png)


The scatter plot and OLS line for predictions vs. average sale per transaction demonstrates the final regression model by liquor category. The model yielded adjusted R-squared of .768 and MSE of 404.40. Liquor, whiskey, vodka, tequila, and schnapps had the highest average sale per transaction, indicating that these types if liquor are more likely to sell compared to the other types of liquor. 


![OLS and Scatter]({{ site.url }}{{ site.baseurl }}/images/pic2_5.png){: .align-center}


### Assumptions of Linear Regression

Before finalizing the final model, we need to check whether our regression model meets four linear regression assumptions: 1) linearity and additivity of the relationship between dependent and independent variables, 2) statistical independence of the errors, 3) normality of the error distribution, and 4) homoscedasticity (constant variance) of the errors.

As the residuals vs. fitted values plot below indicates, there is a linear relationship between predictive values and sale. The Durbin – Watson (DW) statistics in the table above indicates that the regression model has a very slight positive autocorrelation with DW =1.70, but this value is not large enough to discard the model.  Thus, we can say that the final regression model met the assumption of statistical independence. The probability plot of residuals and the histogram of residuals bellow demonstrates no indication of abnormal distribution of errors. However, the residuals vs. fitted values plot obviously signals the presence of homoscedasticity as indicated by the funnel shape of the scatter points.


![Regression Assumptions]({{ site.url }}{{ site.baseurl }}/images/pic2_6.png){: .align-center}


Before finalizing the final model, we need to check whether our regression model meets four linear regression assumptions: 1) linearity and additivity of the relationship between dependent and independent variables, 2) statistical independence of the errors, 3) normality of the error distribution, and 4) homoscedasticity (constant variance) of the errors.

As the residuals vs. fitted values plot below indicates, there is a linear relationship between predictive values and sale. The Durbin – Watson (DW) statistics in the table above indicates that the regression model has a very slight positive autocorrelation with DW =1.70, but this value is not large enough to discard the model.  Thus, we can say that the final regression model met the assumption of statistical independence. The probability plot of residuals and the histogram of residuals bellow demonstrates no indication of abnormal distribution of errors. However, the residuals vs. fitted values plot obviously signals the presence of homoscedasticity as indicated by the funnel shape of the scatter points.

Since the final regression model did not meet the assumptions of homoscedasticity, it could not be used as a final profitability model for the Iowa liquor stores. Instead, I ran and cross-validated several non-parametric repressors: Decision Tree, Bagging DT, Random Forest, Ada Boost, Gradient Boosting, and Neural Network. I used Negative Absolute Median Error (NAME) as the cross-validation score, and there were some fluctuations in the score. As seen below, ada boost algorithm yielded slightly lower accuracy score than the other algorithms. 

![Table with Scores]({{ site.url }}{{ site.baseurl }}/images/table2_2.png){: .align-center}


I chose the decision tree repressor and ada boost repressor as my final non-parametric profitability models. The decision tree model was one of my choices because in spite of its propensity to overfitting, it is the easiest model to use and interpret (after the linear regression), performs well with a large set of data, has higher training and prediction speed.

![Decision Tree]({{ site.url }}{{ site.baseurl }}/images/pic2_8.png){: .align-center} 


However, as seen below decision tree model perfectly fits the data, and it is a sign of overfiiting. 


![Decision Tree]({{ site.url }}{{ site.baseurl }}/images/pic2_7.png){: .align-center} 


The ada boost algorithm is more robust to overfitting and yields more accurate results, but much harder to interpret.


### Conclusion

































