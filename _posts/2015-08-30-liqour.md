---
title: "Building a Profitability Model for an Iowa Liqour Store"
excerpt: "This post covers a predictive model for Iowa liqour stores"
header:
  overlay_image: liqour2.jpg
  overlay_filter: 0.4
  caption: ""
  cta_label: " "
categories:
  - data science
tags:
  - profitability model
  - multiple regression
  - linear regression 
  - decision tree regressor
  - linear regression assumptions
author: "Marina Drus"
date: "30 October 2016"
---

{% include base_path %}

### Introduction

The goal of this project was to build a profitability model based on total spirits sales in Iowa at the store level. Since it is a very large data set, only ten percent of the data was retrieved from [the Iowa website](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy). The dataset contains the spirits purchase information of Iowa Class “E” liquor licensees by product and date of purchase. For the variables’ description, please visit [the Iowa website](https://data.iowa.gov/Economy/Iowa-Liquor-Sales/m3tr-qhgy).

The data contains total sale of liquor ordered per transaction bought by each store from the state of Iowa. That is right. The data does not contain total sale of liquor for each transaction made by customers in each store. Thus, while exploring the data, a very important assumption was made that owners in Iowa are knowledgeable business people, and they only stock what they sell. I really hope so, otherwise they would probably go out of business. In addition to the data I had, two new variables were created: Store (a number of stores per each zip code) and Vendor (a number of vendors per each zip code). 


### Multiple Regression Model

Since the outcome variable Sale is continuous, a multiple regression model was built. I used stepwise approach in building the regression model. However, first and foremost, I looked at average sales per transaction that are outliers. As seen below from the 
[histogram](https://en.wikipedia.org/wiki/Histogram) and [Q-Q plot]9https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot), average costs per transaction that exceeded $250 were exceptions. 


![Histogram and QQ Plot With Outliers]({{ site.url }}{{ site.baseurl }}/images/pic2_1.png){: .align-center} 


After removing the outliers, the distribution of average cost per transaction looked much closer to [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). See the histogram and Q-Q plot below.


![Histogram and QQ Plot Without Outliers]({{ site.url }}{{ site.baseurl }}/images/pic2_2.png){: .align-center} 


Finally, I got to conduct a [stepwise regression](https://en.wikipedia.org/wiki/Stepwise_regression) to select my final linear regression model. Stepwise regression is a method of fitting regression models step-by-step. In each step, I consider a variable for addition to or subtraction from the set of all variable based on an adjusted R-squared. If I saw no changes in an adjusted R-squared, I would not include that variable into the statistical model. I also looked at multicollearity, the highly correlated independent variables were removed from the model. 


![Correlational Matrix]({{ site.url }}{{ site.baseurl }}/images/pic2_3.png){: .align-center}


I excluded variable Cost (cost per transaction) as it was highly correlated with Retail (retail price per transaction). Because Store (a number of stores per each zip code) and Vendor (a number of vendors per each zip code) highly correlated (r=.73), I also removed Store out of my model. Additionally, I checked whether Store affected in anyway the adjusted R-squared and found no changes there. 

Since the variable Month did not predict Sale, the final model excluded the month when liquor was purchased. Consequentially, I adjusted density of the data points by removing category Month from my data. 


![Scatters for Density]({{ site.url }}{{ site.baseurl }}/images/pic2_4.png){: .align-center}


After regrouping the data [MSE (mean squared error)](https://en.wikipedia.org/wiki/Mean_squared_error) of the final regression model was substantially decreased.

As seen in the regression table below, all independent variables positively and significantly predicted average sale per transaction. Sale per transaction increased as Retail (retail price per bottle), Sold (a number of bottles sold), Vendor (a number of vendors per zip code), and Volume (bottle volume) increased. All categories of liquor (expect schnapps) significantly predicted sale per transaction compared to brandy (the baseline category). 


![Regression Table]({{ site.url }}{{ site.baseurl }}/images/table2_1.png)


The scatter plot and OLS line for predictions vs. average sale per transaction demonstrates the final regression model by liquor category. The model yielded adjusted R-squared of .768 and MSE of 404.40. Liquor, whiskey, vodka, tequila, and schnapps had the highest average sale per transaction, indicating that these types if liquor are more likely to sell compared to the other types of liquor. 


![OLS and Scatter]({{ site.url }}{{ site.baseurl }}/images/pic2_5.png){: .align-center}


### Assumptions of Linear Regression

Before finalizing the final model, I checked whether the regression model meets 4 linear regression assumptions: 1) linearity and additivity of the relationship between dependent and independent variables, 2) statistical independence of the errors, 3) normality of the error distribution, and 4) homoscedasticity (constant variance) of the errors.


![Regression Assumptions]({{ site.url }}{{ site.baseurl }}/images/pic2_6.png){: .align-center}


As the residuals vs. fitted values plot below indicates, there is a little skewness in linear relationship between predicted values and sale, but it is not large enough to discard the model. The Durbin – Watson (DW) statistics in the table above indicates that the regression model has a very slight positive autocorrelation with DW =1.70, but this value is also not large enough to discard the model.  Thus, we can say that the final regression model met the assumptions of linearity and statistical independence. The probability plot of residuals and the histogram of residuals bellow demonstrates no indication of abnormal distribution of errors. However, the residuals vs. fitted values plot obviously signals the presence of homoscedasticity as indicated by the funnel shape of the scatter points.

Since the final regression model did not meet the assumptions of homoscedasticity and barely met the assumptions of the assumptions of linearity and statistical independence, it could not be used as a final profitability model for Iowa liquor stores. Instead, I ran and cross-validated several non-parametric repressors: Decision Tree, Bagging DT, Random Forest, Ada Boost, Gradient Boosting, and Neural Network. I used MSE as a cross-validation score, and there were some fluctuations in the scores. As seen below, ada boost algorithm yielded slightly lower accuracy score than the other algorithms.
 

<figure style="width: 300px" class="align-center">
![Table with Scores]({{ site.url }}{{ site.baseurl }}/images/table2_2.png)
</figure>


<figure style="width: 300px" class="align-center">
  <img src="{{ site.url }}{{ site.baseurl }}/images/table2_2.png" alt="">
</figure>


I chose the decision tree repressor and ada boost repressor as my final non-parametric profitability models. The decision tree model was one of my choices because in spite of its propensity to overfitting, it is the easiest model to use and interpret (after the linear regression), performs well with a large set of data, has higher training and prediction speed.

![Decision Tree]({{ site.url }}{{ site.baseurl }}/images/pic2_8.png){: .align-center} 


However, as seen below decision tree model perfectly fits the data, and it is a sign of overfiiting. 


![Decision Tree]({{ site.url }}{{ site.baseurl }}/images/pic2_7.png){: .align-center} 


The ada boost algorithm is more robust to overfitting and yields more accurate results, but much harder to interpret.


### Conclusion

































